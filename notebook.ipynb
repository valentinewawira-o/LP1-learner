{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indian startup ecosystem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your team is trying to venture into the Indian start-up ecosystem. As the data expert of the team, you are to investigate the ecosystem and propose the best course of action.\n",
    "Analyze funding received by start-ups in India from 2018 to 2021.\n",
    "Separate data for each year of funding will be provided.\n",
    "â€¢ In these datasets, you'll find the start-ups' details, the funding amounts received, and the investors' information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 loading packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from dotenv import dotenv_values\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import f_oneway\n",
    "import statistics as stat\n",
    "import warnings\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 load environment variables and setting the .env files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file into a dictionary\n",
    "environment_variables = dotenv_values('.env')\n",
    "\n",
    "\n",
    "# Get the values for the credentials you set in the '.env' file\n",
    "server = environment_variables.get(\"SERVER\")\n",
    "database = environment_variables.get(\"DATABASE\")\n",
    "username = environment_variables.get(\"USERNAME\")\n",
    "password = environment_variables.get(\"PASSWORD\")\n",
    "\n",
    "connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password};MARS_Connection=yes;MinProtocolVersion=TLSv1.2;\"\n",
    "\n",
    "connection =pyodbc.connect(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Expected Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "&#8226; business understanding\n",
    "\n",
    "&#8226; load packages\n",
    "\n",
    "&#8226; load dataset\n",
    "\n",
    "&#8226; clean datasets\n",
    "\n",
    "&#8226; test hypothesis\n",
    "\n",
    "&#8226; answer question\n",
    "\n",
    "&#8226; insights and conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>load the 2020&2021 sql dataset<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 2020 sql dataset\n",
    "query_2020= \"Select * FROM dbo.LP1_startup_funding2020\"\n",
    "data_2020 = pd.read_sql(query_2020, connection)\n",
    "\n",
    "#load the 2021 sql datasets\n",
    "query_2021= \"Select * FROM dbo.LP1_startup_funding2021\"\n",
    "data_2021= pd.read_sql(query_2021, connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> load the 2018 & 2019 csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load 2018 csv dataset\n",
    "data_2018=pd.read_csv('startup_funding2018.csv')\n",
    "\n",
    "#load data 2019 csv files\n",
    "data_2019=pd.read_csv('startup_funding2019.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cleaning the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step 1 - add and drop some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a Year Column (this will help in concating and merging the data sets)\n",
    "data_2018['Year']='2018'\n",
    "data_2019['Year']='2019'\n",
    "data_2020['Year']='2020'\n",
    "data_2021['Year']='2021'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the column we will not be using from data 2019,2020 and 2021\n",
    "data_2019.drop(['Founded', 'Founders', 'Investor'], axis=1, inplace=True)\n",
    "data_2020.drop(['Founded', 'Founders', 'Investor'], axis=1, inplace=True)\n",
    "data_2021.drop(['Founded', 'Founders', 'Investor'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step 2 - renaming common columns into common names  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming the 2018 dataset columns\n",
    "data_2018=data_2018.rename(columns={'Company Name': 'Company_Brand', 'Round/Series': 'Stage',\n",
    "                                    'Industry': 'Sector', 'Amount': 'Amount', 'About Company': 'BIO', 'Location':'HeadQuarter'})\n",
    "\n",
    "# renaming the 2019 dataset columns\n",
    "data_2019 = data_2019.rename(columns={'Company/Brand': 'Company_Brand', 'Sector': 'Sector', 'Stage': 'Stage', 'Amount($)': 'Amount',\n",
    "                                      'What it does': 'BIO', 'HeadQuarter':'HeadQuarter'})\n",
    "\n",
    "# Renaming the 2020 dataset\n",
    "\n",
    "data_2020 = data_2020.rename(columns={'Company_Brand': 'Company_Brand', 'Sector': 'Sector', 'Amount': 'Amount',\n",
    "                                       'What_it_does': 'BIO', 'Location':'HeadQuarter'})\n",
    "\n",
    "#renaming  the  2021 dataset \n",
    "data_2021 = data_2021.rename(columns={'Company_Brand': 'Company_Brand', 'Sector': 'Sector', 'Amount': 'Amount', \n",
    "                                      'What_it_does': 'BIO', 'HeadQuarter':'HeadQuarter'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step 3 - Harmonising the Stage column entries in 2018,2019,2020 &2021 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stage\n",
       "Seed           280\n",
       "Series A        73\n",
       "Undisclosed     39\n",
       "Angel           37\n",
       "Series B        20\n",
       "Other           17\n",
       "Series C        16\n",
       "Debt            15\n",
       "Equity          13\n",
       "Pre-Seed         6\n",
       "Series E         5\n",
       "Series D         3\n",
       "Series H         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2018['Stage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  # replacing the data 2018 unique satage value to a common value\n",
    "data_2018.replace(to_replace=['Seed round', 'Seed funding', 'Early seed', 'Seed fund', 'Seed Investment', 'Seed Round', 'Seed+'], value='Seed', inplace=True)\n",
    "data_2018.replace(to_replace=['Angel Round'], value='Angel', inplace=True)\n",
    "data_2018.replace(to_replace=['Venture - Series Unknown'], value='Undisclosed', inplace=True)\n",
    "data_2018.replace(to_replace=['Debt Financing', 'Post-IPO Debt'], value='Debt', inplace=True)\n",
    "data_2018.replace(to_replace=['Private Equity', 'Post-IPO Equity'], value='Equity', inplace=True)\n",
    "data_2018.replace(to_replace=['Corporate Round', 'Grant', 'Secondary Market', 'Non-equity Assistance', 'Funding Round'], value='Other', inplace=True)\n",
    "data_2018.replace(to_replace=['Pre-Series B', 'Pre-series B'], value='Pre-Seed', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove non-standard entries (https)\n",
    "\n",
    "data_2018= data_2018[data_2018['Stage'].str.startswith('https')==False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stage\n",
       "Series A    10\n",
       "Pre-Seed     9\n",
       "Series B     9\n",
       "Series D     4\n",
       "Series C     3\n",
       "Seed         3\n",
       "Other        2\n",
       "Series G     1\n",
       "Series E     1\n",
       "Series F     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2019['Stage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the data 2019 unique satage value to a common value\n",
    "\n",
    "data_2019.replace(to_replace=['Seed funding', 'Seed fund', 'Seed round'], value='Seed', inplace=True)\n",
    "data_2019.replace(to_replace=['Pre series A', 'Pre-series A'], value='Pre-Seed', inplace=True)\n",
    "data_2019.replace(to_replace=['Series B+'], value='Series B', inplace=True)\n",
    "data_2019.replace(to_replace=['Post series A', 'Fresh funding'], value='Other', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stage\n",
       "Seed                     173\n",
       "Pre-Series               133\n",
       "Series A                  97\n",
       "Series B                  59\n",
       "Series C                  50\n",
       "Series D                  24\n",
       "Debt                      18\n",
       "Pre-seed                  11\n",
       "Series E and Beyond       11\n",
       "Bridge                     8\n",
       "Angel                      4\n",
       "Series H                   1\n",
       "Seed Round & Series A      1\n",
       "Mid series                 1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2020['Stage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stage\n",
       "Seed                     173\n",
       "Pre-Series               133\n",
       "Series A                  97\n",
       "Series B                  59\n",
       "Series C                  50\n",
       "Series D                  24\n",
       "Debt                      18\n",
       "Pre-seed                  11\n",
       "Series E and Beyond       11\n",
       "Bridge                     8\n",
       "Angel                      4\n",
       "Series H                   1\n",
       "Seed Round & Series A      1\n",
       "Mid series                 1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2020['Stage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the data 2020 unique satage value to a common value\n",
    "\n",
    "data_2020.replace(to_replace=['Seed Round', 'Seed round', 'Seed funding', 'Seed A', 'Seed Funding', 'Seed Investment','Pre seed Round', 'Seed funding' 'Seed A' 'Pre-seed', 'Pre seed round', 'Pre-seed Round', 'Pre-Seed'], value='Seed', inplace=True)\n",
    "data_2020.replace(to_replace=['Series A1', 'Series A-1'], value='Series A', inplace=True)\n",
    "data_2020.replace(to_replace=['Series B2'], value='Series B', inplace=True)\n",
    "data_2020.replace(to_replace=['Pre series B', 'Pre-Series B', 'Pre series C', 'Pre-Series B' 'Pre series C','Pre series A1', 'Pre-series', 'Pre series A', 'Pre-series A', 'Pre- series A', 'Pre Series A', 'Pre-series A1', 'Pre-series C', 'Pre-series B'], value='Pre-Series', inplace=True)\n",
    "data_2020.replace(to_replace=['Series E', 'Series E2', 'Series F'], value='Series E and Beyond', inplace=True)\n",
    "data_2020.replace(to_replace=['Series C, D', 'Series D', 'Series D1'], value='Series D', inplace=True)\n",
    "data_2020.replace(to_replace=['Bridge Round', 'Edge', 'Bridge'], value='Bridge', inplace=True)\n",
    "data_2020.replace(to_replace=['Angel Round'], value='Angel', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stage\n",
       "Seed                   295\n",
       "Pre-Series             173\n",
       "Series A               129\n",
       "Series B                50\n",
       "Series C                45\n",
       "Series E and Beyond     31\n",
       "Debt                    27\n",
       "Series D                22\n",
       "Angel                    6\n",
       "Bridge                   2\n",
       "PE                       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2021['Stage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the data 2021 unique satage value to a common value\n",
    "\n",
    "data_2021.replace(to_replace=['Seed Round', 'Seed round', 'Seed funding', 'Seed A', 'Seed+', 'Seed Funding', 'Seed Investment','Pre seed Round', 'Early seed', 'Seed funding' 'Seed A' 'Pre-seed', 'Pre seed round', 'Pre-seed', 'Pre-seed Round', 'Pre-Seed'], value='Seed', inplace=True)\n",
    "data_2021.replace(to_replace=['Series A1', 'Seies A', 'Series A2', 'Series A+', 'Series A-1'], value='Series A', inplace=True)\n",
    "data_2021.replace(to_replace=['Series B2', 'Series B3'], value='Series B', inplace=True)\n",
    "data_2021.replace(to_replace=['Pre series B', 'Pre-Series B', 'Pre series C', 'Pre-Series B' 'Pre series C','Pre series A1', 'Pre-series', 'Pre series A', 'Pre-series A', 'Pre- series A', 'Pre Series A', 'Pre-series A1', 'Pre-series C', 'Pre-series B'], value='Pre-Series', inplace=True)\n",
    "data_2021.replace(to_replace=['Series E', 'Series I', 'Series F1', 'Series H', 'Series G', 'Series F2', 'Series E2', 'Series F'], value='Series E and Beyond', inplace=True)\n",
    "data_2021.replace(to_replace=['Series C, D', 'Series D', 'Series D1'], value='Series D', inplace=True)\n",
    "data_2021.replace(to_replace=['Bridge Round', 'Edge', 'Bridge'], value='Bridge', inplace=True)\n",
    "data_2021.replace(to_replace=['Angel Round', '$300000', '$1200000', '$6000000', '$1000000'], value='Angel', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step 3 - Harmonising the Headquater/location column entries in 2018,2019,2020&2021 data.<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harmonising the 2018 headquater columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeadQuarter\n",
       "Bangalore, Karnataka, India         101\n",
       "Mumbai, Maharashtra, India           94\n",
       "Bengaluru, Karnataka, India          55\n",
       "Gurgaon, Haryana, India              52\n",
       "New Delhi, Delhi, India              51\n",
       "Pune, Maharashtra, India             20\n",
       "Chennai, Tamil Nadu, India           19\n",
       "Hyderabad, Andhra Pradesh, India     18\n",
       "Delhi, Delhi, India                  16\n",
       "Noida, Uttar Pradesh, India          15\n",
       "Haryana, Haryana, India              11\n",
       "Jaipur, Rajasthan, India              9\n",
       "Ahmedabad, Gujarat, India             6\n",
       "Kolkata, West Bengal, India           6\n",
       "Bangalore City, Karnataka, India      5\n",
       "Indore, Madhya Pradesh, India         4\n",
       "India, Asia                           4\n",
       "Kormangala, Karnataka, India          3\n",
       "Ghaziabad, Uttar Pradesh, India       2\n",
       "Kochi, Kerala, India                  2\n",
       "Bhopal, Madhya Pradesh, India         2\n",
       "Thane, Maharashtra, India             2\n",
       "Jodhpur, Rajasthan, India             1\n",
       "Powai, Assam, India                   1\n",
       "Andheri, Maharashtra, India           1\n",
       "Mylapore, Tamil Nadu, India           1\n",
       "Kalpakkam, Tamil Nadu, India          1\n",
       "Guntur, Andhra Pradesh, India         1\n",
       "Coimbatore, Tamil Nadu, India         1\n",
       "Worli, Maharashtra, India             1\n",
       "Alleppey, Kerala, India               1\n",
       "Chandigarh, Chandigarh, India         1\n",
       "Guindy, Tamil Nadu, India             1\n",
       "Uttar Pradesh, India, Asia            1\n",
       "Trivandrum, Kerala, India             1\n",
       "Ernakulam, Kerala, India              1\n",
       "Kanpur, Uttar Pradesh, India          1\n",
       "Kannur, Kerala, India                 1\n",
       "Alwar, Rajasthan, India               1\n",
       "MargÃ£o, Goa, India                    1\n",
       "Belgaum, Karnataka, India             1\n",
       "Kalkaji, Delhi, India                 1\n",
       "Anand, Gujarat, India                 1\n",
       "Kota, Rajasthan, India                1\n",
       "Faridabad, Haryana, India             1\n",
       "Cochin, Kerala, India                 1\n",
       "Hubli, Karnataka, India               1\n",
       "Azadpur, Delhi, India                 1\n",
       "Mohali, Punjab, India                 1\n",
       "Lucknow, Uttar Pradesh, India         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2018['HeadQuarter']. value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning of headquarter in 2018\n",
    "\n",
    "data_2018.replace(to_replace=['Bangalore City','Bangalore'], value='Bangalore', inplace=True)\n",
    "data_2018.replace(to_replace=['Delhi','New Delhi'], value='Delhi', inplace=True)\n",
    "data_2018.replace(to_replace=['Cochin'], value='Kochi', inplace=True)\n",
    "data_2018.replace(to_replace=['Kormangala'], value='Koramangala', inplace=True)\n",
    "data_2018.replace(to_replace=['Powai','Worli'], value='Mumbai', inplace=True)\n",
    "data_2018.replace(to_replace=['Uttar Pradesh', 'Andhra Pradesh'], value='Andhra Pradesh', inplace=True)\n",
    "data_2018.replace(to_replace=['Trivandrum', 'India'], value='Trivandrum', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Harmonize the 2019 data headquater column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeadQuarter\n",
       "Bangalore        21\n",
       "Mumbai           12\n",
       "Delhi            11\n",
       "Noida             5\n",
       "Gurugram          5\n",
       "Chennai           4\n",
       "Pune              2\n",
       "Jaipur            2\n",
       "Telangana         1\n",
       "Ahmedabad         1\n",
       "Haryana           1\n",
       "Chandigarh        1\n",
       "Surat             1\n",
       "Uttar pradesh     1\n",
       "Hyderabad         1\n",
       "Rajasthan         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2019['HeadQuarter'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2019 headquater data cleaning\n",
    "data_2019.replace(to_replace=['Delhi' , 'New Delhi'], value='Delhi', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Harmonizing the 2020 data headquater column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.value_counts of 0           Chennai\n",
       "1         Bangalore\n",
       "2              Pune\n",
       "3             Delhi\n",
       "4            Indore\n",
       "           ...     \n",
       "1050          Delhi\n",
       "1051    Undisclosed\n",
       "1052         Mumbai\n",
       "1053          Delhi\n",
       "1054        Chennai\n",
       "Name: HeadQuarter, Length: 1055, dtype: object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2020['HeadQuarter']. value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  cleaning of HeadQuater 2020\n",
    "data_2020.replace(to_replace=['Bangalore City','Bangalore'], value='Bangalore', inplace=True)\n",
    "data_2020.replace(to_replace=['Delhi','New Delhi'], value='Delhi', inplace=True)\n",
    "data_2020.replace(to_replace=['Ahmadabad'], value='Ahmedabad', inplace=True)\n",
    "data_2020.replace(to_replace=['Kochi'], value='Cochin', inplace=True)\n",
    "data_2020.replace(to_replace=['Kormangala'], value='Koramangala', inplace=True)\n",
    "data_2020.replace(to_replace=['Rajastan'], value='Rajasthan', inplace=True)\n",
    "data_2020.replace(to_replace=['Powai','Worli'], value='Mumbai', inplace=True)\n",
    "data_2020.replace(to_replace=['Small Towns', 'Andhra Pradesh','Uttar Pradesh'], value='Andhra Pradesh', inplace=True)\n",
    "data_2020.replace(to_replace=['Hyderebad'], value='Hyderabad', inplace=True)\n",
    "data_2020.replace(to_replace=['Gurugram\\t#REF!'], value='Gurugram', inplace=True)\n",
    "data_2020.replace(to_replace=['Orissia'], value='Orissa', inplace=True)\n",
    "data_2020.replace(to_replace=['Samstipur','Samastipur, Bihar','Samsitpur'], value='Samastipur', inplace=True)\n",
    "data_2020.replace(to_replace=['The Nilgiris'], value='Nilgiris', inplace=True)\n",
    "data_2020.replace(to_replace=['Dhindsara', 'Haryana','Dhingsara, Haryana'], value='Dhingsara', inplace=True)\n",
    "data_2020.replace(to_replace=['Tirunelveli', 'Tamilnadu'], value='Tirunelveli', inplace=True)\n",
    "data_2020.replace(to_replace=['Mylapore'], value='Chennai', inplace=True)\n",
    "data_2020.replace(to_replace=['Rajastan'], value='Rajasthan', inplace=True)\n",
    "data_2020.replace(to_replace=['Trivandrum', 'Kerala, India'], value='Trivandrum', inplace=True)\n",
    "\n",
    "#  Renaming some of the headquarters the are not in India to outside india in data 2020\n",
    "data_2020.replace(to_replace=['Frisco', 'France','Newcastle Upon Tyne, Newcastle upon Tyne, United Kingdom','Frisco Texas, United States',\n",
    "                              'Irvine, California, United States','San Francisco Bay Area, West Coast, Western US','Texas, United States',\n",
    "                              'California','New York, United States','San Francisco, California, United States','San Francisco, United States',\n",
    "                              'San Ramon, California','Paris, Ile-de-France, France','Plano, Texas, United States','Sydney',\n",
    "                              'San Francisco Bay Area, Silicon Valley, West Coast','Bangaldesh','London, England, United Kingdom',\n",
    "                              'Sydney, New South Wales, Australia','Milano, Lombardia, Italy','Palmwoods, Queensland, Australia', \n",
    "                              'France''Irvine, California, United States','Newcastle Upon Tyne, Newcastle upon Tyne United Kingdom',\n",
    "                              'Shanghai, China','Jiaxing, Zhejiang, China','San Franciscao','San Francisco','New York'], \n",
    "                              value = 'Outside India', inplace=True)\n",
    "\n",
    "# filling of missing headquarters in 2020\n",
    "data_2020['HeadQuarter'].fillna('Undisclosed', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Harmonizing the 2021 headquater data colummn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bangalore', 'Mumbai', 'Gurugram', 'New Delhi', 'Hyderabad',\n",
       "       'Jaipur', 'Ahmadabad', 'Chennai', None,\n",
       "       'Small Towns, Andhra Pradesh', 'Goa', 'Rajsamand', 'Ranchi',\n",
       "       'Faridabad, Haryana', 'Gujarat', 'Pune', 'Thane', 'Undisclosed',\n",
       "       'Cochin', 'Noida', 'Chandigarh', 'Gurgaon', 'Vadodara',\n",
       "       'Gurugrama', 'Kolkata', 'Ahmedabad', 'Mohali', 'Haryana', 'Indore',\n",
       "       'Powai', 'Ghaziabad', 'Nagpur', 'West Bengal', 'Patna',\n",
       "       'Samsitpur', 'Lucknow', 'Telangana', 'Silvassa',\n",
       "       'Thiruvananthapuram', 'Faridabad', 'Roorkee', 'Ambernath',\n",
       "       'Panchkula', 'Surat', 'Coimbatore', 'Andheri', 'Mangalore',\n",
       "       'Telugana', 'Bhubaneswar', 'Kottayam', 'Outside_India', 'Panaji',\n",
       "       'Satara', 'Orissia', 'Jodhpur', 'Santra', 'Mountain View, CA',\n",
       "       'Trivandrum', 'Jharkhand', 'Kanpur', 'Bhilwara', 'Guwahati',\n",
       "       'Kochi', 'The Nilgiris', 'Gandhinagar'], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2021['HeadQuarter'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2021 headquater data cleaning\n",
    "data_2021.replace(to_replace=['London', 'New York', ' Mountain View', 'CA ', 'Beijing', '' ], value='Outside_India', inplace=True)\n",
    "data_2021.replace(to_replace=['Online Media\\t#REF!', 'Pharmaceuticals\\t#REF!', 'Food & Beverages', \n",
    "                             'Computer Games', 'Small Towns', ' None', 'None', 'Information Technology & Services'], value='Undisclosed', inplace=True)\n",
    "data_2021.replace(to_replace=['Gurugram\\t#REF!' ], value='Gurugrama', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Currency Conversion for 2018,2019,2020 & 2021 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Currency Conversion For 2018 & 2019 csv Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two csv files \n",
    "data2018_2019 =pd.concat([data_2018,data_2019], ignore_index= True)\n",
    "\n",
    "\n",
    "# Create a new column 'currency' based on currency symbols\n",
    "data2018_2019['currency']= data2018_2019['Amount'].apply(lambda x: 'INR' if 'â‚¹' else ('USD' if '$'in x else 'USD'))\n",
    "\n",
    "\n",
    "# Create columns 'amount_inr' and 'amount_usd'\n",
    "\n",
    "data2018_2019['amount_inr'] = data2018_2019.apply(lambda row: row['Amount'] if row['currency'] == 'INR' else 0, axis=1)\n",
    "data2018_2019['amount_usd'] = data2018_2019.apply(lambda row: row['Amount'] if row['currency'] == 'USD' else 0, axis=1)\n",
    "\n",
    "# Remove the currency sysmbols and the commas \n",
    "\n",
    "data2018_2019['amount_inr'] = data2018_2019['amount_inr'].str.replace('â‚¹', '').str.replace('$', '').str.replace(',', '')\n",
    "data2018_2019['amount_usd']= data2018_2019['amount_usd'].str.replace('â‚¹','').str.replace('$', '').str.replace(',','')\n",
    "\n",
    "\n",
    "# Replace 'Undisclosed' values with NaN\n",
    "data2018_2019['amount_inr']= data2018_2019['amount_inr'].replace('Undisclosed',np.nan, regex=True)\n",
    "data2018_2019['amount_usd'] = data2018_2019['amount_usd'].replace('Undisclosed', np.nan, regex=True)\n",
    "\n",
    "# Convert the object values into numeric values for calculation\n",
    "\n",
    "data2018_2019['amount_inr'] = pd.to_numeric(data2018_2019['amount_inr'], errors='coerce')\n",
    "data2018_2019['amount_usd']= pd.to_numeric(data2018_2019['amount_usd'],errors=['coerce'])\n",
    "\n",
    "# Define the exchange rate\n",
    "exchange_rate_inr_to_usd = 1 / 83.23  # 1 USD = 83.23 INR\n",
    "\n",
    "# Convert 'amount to USD\n",
    "data2018_2019['amount_inr']= data2018_2019['amount_inr'].apply(lambda x: x *exchange_rate_inr_to_usd if x is not None else None)\n",
    "data2018_2019['amount_inr'] = data2018_2019['amount_inr'].apply(lambda x: x * exchange_rate_inr_to_usd if x is not None else None\n",
    "\n",
    "# Create a new column 'Updated_Amount' by filling missing values in 'amount_inr' with 'amount_usd'\n",
    "df18_19['Updated_Amount'] = df18_19['amount_inr'].fillna(df18_19['amount_usd'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
